{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# Process Data\n",
    "<hr>\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pyspark as ps\n",
    "import numpy as np\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.linalg import VectorUDT\n",
    "from sentimentAnalysis import dataProcessing as dp\n",
    "from pyspark.sql.functions import lit, collect_list, col, udf\n",
    "from pyspark.sql.types import ArrayType, StringType, NumericType, DateType\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_category(df_cats):\n",
    "    \"\"\"\n",
    "    INPUT: Spark DataFrame\n",
    "    RETURN: Spark DataFrame\n",
    "    \n",
    "    Takes in a DataFrame with a wrapped array \"categories\"\n",
    "    column. Extracts first category, drops \"categories\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # create udf\n",
    "    extract_cat_udf = udf(lambda x: x[0][0], StringType())\n",
    "\n",
    "    # create new column with single category\n",
    "    df_cat = df_joined.withColumn(\"category\", extract_cat_udf(\"categories\")).drop(\"categories\")\n",
    "    \n",
    "    return df_cat\n",
    "\n",
    "\n",
    "def add_pos_neg_tfidf(df_cat):\n",
    "    \"\"\"\n",
    "    INPUT: Spark DataFrame\n",
    "    RETURN: Spark DataFrame, List, List\n",
    "    \n",
    "    Takes in a DataFrame with review column overall.\n",
    "    Splits into postitive and negative reviews, adds\n",
    "    TFIDF vectors for each subset. Returns joined\n",
    "    DataFrame with all data, list of postive review\n",
    "    vocabulary terms, and list of negative review\n",
    "    vocabulary terms\n",
    "    \n",
    "    \"\"\"\n",
    "    # separate positive and negative reviews\n",
    "    df_pos = df_cat.where(df_cat.overall >= 4.0).withColumn(\"positive\", lit(True))\n",
    "    df_neg = df_cat.where(df_cat.overall <= 2.0).withColumn(\"positive\", lit(False))\n",
    "    \n",
    "    # tokenize\n",
    "    df_pos_tk = dp.add_tokens(df_pos).select(\"asin\",\n",
    "                                             \"category\",\n",
    "                                             \"overall\",\n",
    "                                             \"positive\",\n",
    "                                             \"reviewerName\",\n",
    "                                             \"unixReviewTime\",\n",
    "                                             \"reviewText\",\n",
    "                                             \"tokens\")\n",
    "    \n",
    "    df_neg_tk = dp.add_tokens(df_neg).select(\"asin\",\n",
    "                                             \"category\",\n",
    "                                             \"overall\",\n",
    "                                             \"positive\",\n",
    "                                             \"reviewerName\",\n",
    "                                             \"unixReviewTime\",\n",
    "                                             \"reviewText\",\n",
    "                                             \"tokens\")\n",
    "    \n",
    "    # get tf, vocab\n",
    "    df_tf_pos, vocab_pos = dp.add_tf_and_vocab(df_pos_tk)\n",
    "    df_tf_neg, vocab_neg = dp.add_tf_and_vocab(df_neg_tk)\n",
    "    \n",
    "    # add tfidf\n",
    "    df_tfidf_pos = dp.add_tfidf(df_tf_pos).drop(\"tf_vector\").drop(\"tokens\")\n",
    "    df_tfidf_neg = dp.add_tfidf(df_tf_neg).drop(\"tf_vector\").drop(\"tokens\")\n",
    "\n",
    "    \n",
    "    return df_tfidf_pos.unionAll(df_tfidf_neg), vocab_pos, vocab_neg\n",
    "\n",
    "\n",
    "def rowToJson(rating, date, name, text):\n",
    "    \"\"\"\n",
    "    INPUT: Float, Int, String, String\n",
    "    RETURN: String\n",
    "    \n",
    "    Converts review variables to json string\n",
    "    \n",
    "    \"\"\"\n",
    "    row = { \"rating\": rating,\n",
    "            \"date\": date,\n",
    "            \"name\": name,\n",
    "            \"text\": text\n",
    "          }\n",
    "    \n",
    "    return json.dumps(row)\n",
    "\n",
    "\n",
    "def add_review_col(df):\n",
    "    \"\"\"\n",
    "    INPUT: Spark DataFrame\n",
    "    RETURN: Spark DataFrame\n",
    "    \n",
    "    Adds column with json string representation\n",
    "    of review for each review\n",
    "    \n",
    "    \"\"\"\n",
    "    # create udf\n",
    "    get_review_udf = udf(lambda a,b,c,d: rowToJson(a,b,c,d), StringType())\n",
    "\n",
    "    # create new column with review\n",
    "    df_review = df.withColumn(\"review\", get_review_udf(\"overall\", \n",
    "                                                       \"unixReviewTime\", \n",
    "                                                       \"reviewerName\", \n",
    "                                                       \"reviewText\"))\n",
    "    \n",
    "    return df_review\n",
    "\n",
    "\n",
    "def sum_vectors(vectors):\n",
    "    \"\"\"\n",
    "    INPUT: List of SparseVectors\n",
    "    RETURN: SparseVector\n",
    "    \n",
    "    Sum list of TFIDF vectors element-wise,\n",
    "    return resulting vector\n",
    "    \n",
    "    \"\"\"\n",
    "    # check if vectors exist\n",
    "    if not vectors:\n",
    "        return None\n",
    "    \n",
    "    # iterate over vectors\n",
    "    sum_vector = vectors[0].toArray()\n",
    "    vector_size = sum_vector.shape[0]\n",
    "    \n",
    "    for i,vector in enumerate(vectors[1:]):\n",
    "        sum_vector += vector.toArray()\n",
    "        \n",
    "    # convert to sparse vector   \n",
    "    sparse_vector = SparseVector(vector_size, {i:sum_vector[i] for i in np.nonzero(sum_vector)[0]})\n",
    "    \n",
    "    return sparse_vector\n",
    "\n",
    "\n",
    "def add_vectors_sum(df):\n",
    "    \"\"\"\n",
    "    INPUT: Spark DataFrame\n",
    "    RETURN: Spark DataFrame\n",
    "    \n",
    "    Sum list of TFIDF vectors element-wise for\n",
    "    vectors column, add column for vector sum\n",
    "    \n",
    "    \"\"\"\n",
    "    # create udf\n",
    "    sum_vector_udf = udf(lambda vectors: sum_vectors(vectors), VectorUDT())\n",
    "\n",
    "    # create new column with review\n",
    "    df_vectors_summed = df.withColumn(\"tfidf_vectors_sum\", sum_vector_udf(\"vectors\")).drop(\"vectors\")\n",
    "    \n",
    "    return df_vectors_summed\n",
    "\n",
    "\n",
    "# CARRY OVER FROM dataProcessing - need to be declared locally\n",
    "def test_extract_top_features(tfidf_vector, vocab, n):\n",
    "    \"\"\"\n",
    "    INPUT: SparseVector, List, Int\n",
    "    RETURN: List\n",
    "\n",
    "    Take in TFIDF vector, vocabulary for vector,\n",
    "    and number of terms. Return top n terms\n",
    "\n",
    "    \"\"\"\n",
    "    # note - tfidf elements are pre-sorted by importance\n",
    "    term_indices = tfidf_vector.indices[-n:]\n",
    "\n",
    "    # Map features to terms\n",
    "    features = [vocab[i] for i in term_indices]\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def test_add_top_features(df, vocab, n=10):\n",
    "    \"\"\"\n",
    "    INPUT: PySpark DataFrame, List, Int\n",
    "    RETURN: PySpark DataFrame\n",
    "\n",
    "    Take in DataFrame with TFIDF vectors, list of vocabulary words,\n",
    "    and number of features to extract. Map top features from TFIDF\n",
    "    vectors to vocabulary terms. Return new DataFrame with terms\n",
    "\n",
    "    \"\"\"\n",
    "    # Create udf function to extract top n features\n",
    "    extract_features_udf = udf(lambda x: test_extract_top_features(x, vocab, n))\n",
    "\n",
    "    # Apply udf, create new df with features column\n",
    "    df_features = df.withColumn(\"topFeatures\",\n",
    "                                    extract_features_udf(df[\"tfidf_vectors_sum\"]))\n",
    "\n",
    "\n",
    "    return df_features\n",
    "\n",
    "\n",
    "def test_add_pos_neg_features(df, vocab_pos, vocab_neg, n=10):\n",
    "    \"\"\"\n",
    "    INPUT: Spark DataFrame, List, List, Int\n",
    "    RETURN: Spark DataFrame\n",
    "\n",
    "    Take in DataFrame grouped by asin, positive with tfidf vectors summed.\n",
    "    Extract top positive and negative terms from each group, add features column\n",
    "\n",
    "    \"\"\"\n",
    "    # split dataframe on postitive\n",
    "    df_pos = df.where(df.positive==True)\n",
    "    df_neg = df.where(df.positive==False)\n",
    "\n",
    "    # add features\n",
    "    df_pos_terms = test_add_top_features(df_pos, vocab_pos, n)\n",
    "    df_neg_terms = test_add_top_features(df_neg, vocab_neg, n)\n",
    "\n",
    "    return df_pos_terms.unionAll(df_neg_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collapse_reviews_terms(df):\n",
    "    \"\"\"\n",
    "    INPUT: Spark DataFrame\n",
    "    RETURN: Spark DataFrame\n",
    "\n",
    "    Take in DataFrame with positive and negative reviews,\n",
    "    split and join with columns for positive and negative\n",
    "    features\n",
    "\n",
    "    \"\"\"\n",
    "    # split dataframe\n",
    "    df_pos = df.where(df.positive==True)\n",
    "    df_pos = df_pos.select(col(\"asin\"),\n",
    "                           col(\"category\"),\n",
    "                           col(\"ratings\").alias(\"posRatings\"),\n",
    "                           col(\"reviews\").alias(\"posReviews\"), \n",
    "                           col(\"topFeatures\").alias(\"posFeatures\"))\n",
    "    \n",
    "    df_neg = df.where(df.positive==False)\n",
    "    df_neg = df_neg.select(col(\"asin\"),\n",
    "                           col(\"ratings\").alias(\"negRatings\"),\n",
    "                           col(\"reviews\").alias(\"negReviews\"), \n",
    "                           col(\"topFeatures\").alias(\"negFeatures\"))\n",
    "    \n",
    "    \n",
    "    # get asin \n",
    "    df_asin = df.select(\"asin\").distinct()\n",
    "    \n",
    "    \n",
    "    # join dataframes\n",
    "    df_joined = df_asin.join(df_pos, df_asin.asin==df_neg.asin, 'outer').drop(df_pos.asin)\n",
    "    df_joined = df_joined.join(df_neg, df_joined.asin==df_neg.asin, 'outer').drop(df_neg.asin)\n",
    "    \n",
    "    \n",
    "    return df_joined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create spark session\n",
    "spark = ps.sql.SparkSession.builder \\\n",
    "    .appName(\"reviewProcessing\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get dataframes\n",
    "# specify s3 as sourc with s3a://\n",
    "# df = spark.read.json(\"s3a://amazon-review-data/user_dedup.json.gz\")\n",
    "df_meta = spark.read.json(\"s3a://amazon-review-data/metadata.json.gz\")\n",
    "\n",
    "# get shard\n",
    "df_toys = spark.read.json(\"s3a://amazon-review-data/reviews_Toys_and_Games_5.json.gz\")\n",
    "\n",
    "# subset asin, overall, , reviewerName, reviewText\n",
    "df_subset = df_toys.select(\"asin\", \"overall\", \"reviewerName\", \"unixReviewTime\", \"reviewText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add metadata\n",
    "df_joined = dp.join_metadata(df_subset, df_meta).select(\"asin\",\n",
    "                                                        \"title\",\n",
    "                                                        \"categories\",\n",
    "                                                        \"overall\",\n",
    "                                                        \"reviewerName\",\n",
    "                                                        \"unixReviewTime\",\n",
    "                                                        \"reviewText\").persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove 3 star reviews\n",
    "df_joined_subset = df_joined.where(df_joined.overall != 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extract category, add column\n",
    "df_cat = extract_category(df_joined_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get tfidf vectors and vocabularies\n",
    "df_tfidf, vocab_pos, vocab_neg = add_pos_neg_tfidf(df_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add json review column\n",
    "df_review = add_review_col(df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# group by asin, tfidf_vector\n",
    "df_grouped = df_review.groupBy(\"asin\", \"category\", \"positive\").agg(collect_list(\"review\").alias(\"reviews\"), \n",
    "                                                                   collect_list(\"overall\").alias(\"ratings\"),\n",
    "                                                                   collect_list(\"tfidf_vector\").alias(\"vectors\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sum vector lists\n",
    "df_vectors_summed = add_vectors_sum(df_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add terms\n",
    "# note - udf function that relies on local module functions does not work b/c module does not exist on workers\n",
    "\n",
    "df_terms = test_add_pos_neg_features(df_vectors_summed, vocab_pos, vocab_neg, n=15).drop(\"tfidf_vectors_sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add ratings, collapse df\n",
    "df_ratings = collapse_reviews_terms(df_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "|      asin|    category|          posRatings|          posReviews|         posFeatures|     negRatings|          negReviews|         negFeatures|\n",
      "+----------+------------+--------------------+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "|1556345542|Toys & Games|[4.0, 4.0, 5.0, 5...|[{\"date\": 1023062...|[cheek, humans, c...|           null|                null|                null|\n",
      "|B0007WWZIG|Toys & Games|          [4.0, 5.0]|[{\"date\": 1136246...|[charging, miss, ...|     [1.0, 1.0]|[{\"date\": 1138752...|[charging, we'll,...|\n",
      "|B0007Y4DLG|Toys & Games|[5.0, 5.0, 5.0, 5...|[{\"date\": 1366761...|[purchaser, md, a...|[2.0, 2.0, 1.0]|[{\"date\": 1388620...|[household, assum...|\n",
      "+----------+------------+--------------------+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ratings.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "### Concatenate ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ratings_counter(posRatings, negRatings):\n",
    "    count = Counter(posRatings)\n",
    "    count.update(negRatings)\n",
    "    return count\n",
    "\n",
    "def concatenate_ratings(df):\n",
    "    \"\"\"\n",
    "    INPUT: Spark DataFrame\n",
    "    RETURN: Spark DataFrame\n",
    "    \n",
    "    Takes in a DataFrame with list columns posRatings\n",
    "    and negRatings. Concatenates into ratings column\n",
    "    with counter\n",
    "    \n",
    "    \"\"\"\n",
    "    # create udf\n",
    "    get_ratings_udf = udf(lambda x,y: json.dumps(get_ratings_counter(x,y)), StringType())\n",
    "\n",
    "    # create new column with single category\n",
    "    df_count = df.withColumn(\"ratings\", get_ratings_udf(\"posRatings\", \"negRatings\"))\n",
    "    \n",
    "    return df_count.drop(\"posRatings\").drop(\"negRatings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_ratings_concat = concatenate_ratings(df_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# join titles from df_meta\n",
    "df_final = df_ratings_concat.join(df_meta.select(\"asin\", \"title\"), df_ratings_concat.asin==df_meta.asin).drop(df_meta.asin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|      asin|    category|          posReviews|         posFeatures|          negReviews|         negFeatures|             ratings|               title|\n",
      "+----------+------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|1616613807|Toys & Games|[{\"date\": 1369699...|[evade, rulesheet...|                null|                null|{\"4.0\": 4, \"5.0\":...|Star Wars X-Wing:...|\n",
      "|B00000IZKX|Toys & Games|[{\"date\": 1335916...|[iteration, coil,...|[{\"date\": 1368748...|[2011, slinky, ap...|{\"1.0\": 5, \"2.0\":...|POOF-Slinky Model...|\n",
      "|B000067BIE|Toys & Games|[{\"date\": 1365033...|[talked, exceeded...|                null|                null|{\"4.0\": 2, \"5.0\": 6}|Melissa &amp; Dou...|\n",
      "+----------+------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<hr>\n",
    "## Save DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_final.write.mode('append').json(\"s3a://amazon-review-data/review-data/parts-minDF_5/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11906"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.select(\"asin\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
