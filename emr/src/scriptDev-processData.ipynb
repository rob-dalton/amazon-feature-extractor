{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# Script Development - <code>processData.py</code>\n",
    "Development notebook for script to group by category and asin, split by pos and neg\n",
    "reviews, and add top positive and negative features.\n",
    "<hr>\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "from sentimentAnalysis import dataProcessing as dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'sentimentAnalysis.dataProcessing' from 'sentimentAnalysis/dataProcessing.pyc'>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create spark session\n",
    "spark = ps.sql.SparkSession.builder \\\n",
    "    .appName(\"reviewProcessing\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get dataframes\n",
    "# specify s3 as sourc with s3a://\n",
    "# df = spark.read.json(\"s3a://amazon-review-data/user_dedup.json.gz\")\n",
    "df_meta = spark.read.json(\"s3a://amazon-review-data/metadata.json.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get shard\n",
    "df_mi = spark.read.json(\"s3a://amazon-review-data/reviews_Musical_Instruments_5.json.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subset asin, overall, , reviewerName, reviewText\n",
    "df_subset = df_mi.select(\"asin\", \"overall\", \"reviewerName\", \"unixReviewTime\", \"reviewText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add metadata\n",
    "df_joined = dp.join_metadata(df_subset, df_meta).select(\"asin\",\n",
    "                                                        \"title\",\n",
    "                                                        \"categories\",\n",
    "                                                        \"overall\",\n",
    "                                                        \"reviewerName\",\n",
    "                                                        \"unixReviewTime\",\n",
    "                                                        \"reviewText\").persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<hr>\n",
    "## Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove 3 star reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10261"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count reviews\n",
    "df_joined.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9489"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove reviews\n",
    "df_joined_subset = df_joined.where(df_joined.overall != 3.0)\n",
    "\n",
    "# check reviews were removed\n",
    "df_joined_subset.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Extract category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------+--------------------+--------------+--------------------+\n",
      "|      asin|          categories|overall|        reviewerName|unixReviewTime|          reviewText|\n",
      "+----------+--------------------+-------+--------------------+--------------+--------------------+\n",
      "|1384719342|[WrappedArray(Mus...|    5.0|       SEAN MASLANKA|    1392940800|This pop filter i...|\n",
      "|1384719342|[WrappedArray(Mus...|    5.0|RustyBill \"Sunday...|    1392336000|Nice windscreen p...|\n",
      "|1384719342|[WrappedArray(Mus...|    5.0|Rick Bennette \"Ri...|    1377648000|The primary job o...|\n",
      "+----------+--------------------+-------+--------------------+--------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check df categories\n",
    "df_joined_subset.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType, NumericType, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_category(df_cats):\n",
    "    \"\"\"\n",
    "    INPUT: Spark DataFrame\n",
    "    RETURN: Spark DataFrame\n",
    "    \n",
    "    Takes in a DataFrame with a wrapped array \"categories\"\n",
    "    column. Extracts first category, drops \"categories\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # create udf\n",
    "    extract_cat_udf = udf(lambda x: x[0][0], StringType())\n",
    "\n",
    "    # create new column with single category\n",
    "    df_cat = df_joined.withColumn(\"category\", extract_cat_udf(\"categories\")).drop(\"categories\")\n",
    "    \n",
    "    return df_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+--------------+--------------------+-------------------+\n",
      "|      asin|overall|        reviewerName|unixReviewTime|          reviewText|           category|\n",
      "+----------+-------+--------------------+--------------+--------------------+-------------------+\n",
      "|1384719342|    5.0|       SEAN MASLANKA|    1392940800|This pop filter i...|Musical Instruments|\n",
      "|1384719342|    5.0|RustyBill \"Sunday...|    1392336000|Nice windscreen p...|Musical Instruments|\n",
      "|1384719342|    5.0|Rick Bennette \"Ri...|    1377648000|The primary job o...|Musical Instruments|\n",
      "+----------+-------+--------------------+--------------+--------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test function\n",
    "df_cat = extract_category(df_joined_subset)\n",
    "\n",
    "df_cat.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate pos and neg, add tfidf vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "def add_pos_neg_tfidf(df_cat):\n",
    "    \"\"\"\n",
    "    INPUT: Spark DataFrame\n",
    "    RETURN: Spark DataFrame, List, List\n",
    "    \n",
    "    Takes in a DataFrame with review column overall.\n",
    "    Splits into postitive and negative reviews, adds\n",
    "    TFIDF vectors for each subset. Returns joined\n",
    "    DataFrame with all data, list of postive review\n",
    "    vocabulary terms, and list of negative review\n",
    "    vocabulary terms\n",
    "    \n",
    "    \"\"\"\n",
    "    # separate positive and negative reviews\n",
    "    df_pos = df_cat.where(df_cat.overall >= 4.0).withColumn(\"positive\", lit(True))\n",
    "    df_neg = df_cat.where(df_cat.overall <= 2.0).withColumn(\"positive\", lit(False))\n",
    "    \n",
    "    # tokenize\n",
    "    df_pos_tk = dp.add_tokens(df_pos).select(\"asin\",\n",
    "                                             \"category\",\n",
    "                                             \"overall\",\n",
    "                                             \"positive\",\n",
    "                                             \"reviewerName\",\n",
    "                                             \"unixReviewTime\",\n",
    "                                             \"reviewText\",\n",
    "                                             \"tokens\")\n",
    "    \n",
    "    df_neg_tk = dp.add_tokens(df_neg).select(\"asin\",\n",
    "                                             \"category\",\n",
    "                                             \"overall\",\n",
    "                                             \"positive\",\n",
    "                                             \"reviewerName\",\n",
    "                                             \"unixReviewTime\",\n",
    "                                             \"reviewText\",\n",
    "                                             \"tokens\")\n",
    "    \n",
    "    # get tf, vocab\n",
    "    df_tf_pos, vocab_pos = dp.add_tf_and_vocab(df_pos_tk)\n",
    "    df_tf_neg, vocab_neg = dp.add_tf_and_vocab(df_neg_tk)\n",
    "    \n",
    "    # add tfidf\n",
    "    df_tfidf_pos = dp.add_tfidf(df_tf_pos).drop(\"tf_vector\").drop(\"tokens\")\n",
    "    df_tfidf_neg = dp.add_tfidf(df_tf_neg).drop(\"tf_vector\").drop(\"tokens\")\n",
    "\n",
    "    \n",
    "    return df_tfidf_pos.unionAll(df_tfidf_neg), vocab_pos, vocab_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------+--------+--------------------+--------------+--------------------+--------------------+\n",
      "|      asin|           category|overall|positive|        reviewerName|unixReviewTime|          reviewText|        tfidf_vector|\n",
      "+----------+-------------------+-------+--------+--------------------+--------------+--------------------+--------------------+\n",
      "|1384719342|Musical Instruments|    5.0|    true|       SEAN MASLANKA|    1392940800|This pop filter i...|(19899,[2,3,117,1...|\n",
      "|1384719342|Musical Instruments|    5.0|    true|RustyBill \"Sunday...|    1392336000|Nice windscreen p...|(19899,[22,36,60,...|\n",
      "|1384719342|Musical Instruments|    5.0|    true|Rick Bennette \"Ri...|    1377648000|The primary job o...|(19899,[6,14,16,3...|\n",
      "+----------+-------------------+-------+--------+--------------------+--------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test function\n",
    "df_tfidf, vocab_pos, vocab_neg = add_pos_neg_tfidf(df_cat)\n",
    "\n",
    "df_tfidf.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by product, collect list of reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add review column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import collect_list function\n",
    "from pyspark.sql.functions import collect_list\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create json review from row\n",
    "def rowToJson(rating, date, name, text):\n",
    "    row = { \"rating\": rating,\n",
    "            \"date\": date,\n",
    "            \"name\": name,\n",
    "            \"text\": text\n",
    "          }\n",
    "    \n",
    "    return json.dumps(row)\n",
    "\n",
    "# create review column\n",
    "def add_review_col(df):\n",
    "    # create udf\n",
    "    get_review_udf = udf(lambda a,b,c,d: rowToJson(a,b,c,d), StringType())\n",
    "\n",
    "    # create new column with review\n",
    "    df_review = df.withColumn(\"review\", get_review_udf(\"overall\", \n",
    "                                                       \"unixReviewTime\", \n",
    "                                                       \"reviewerName\", \n",
    "                                                       \"reviewText\"))\n",
    "    \n",
    "    return df_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------+--------+--------------------+--------------+--------------------+--------------------+--------------------+\n",
      "|      asin|           category|overall|positive|        reviewerName|unixReviewTime|          reviewText|        tfidf_vector|              review|\n",
      "+----------+-------------------+-------+--------+--------------------+--------------+--------------------+--------------------+--------------------+\n",
      "|1384719342|Musical Instruments|    5.0|    true|       SEAN MASLANKA|    1392940800|This pop filter i...|(19899,[2,3,117,1...|{\"date\": 13929408...|\n",
      "|1384719342|Musical Instruments|    5.0|    true|RustyBill \"Sunday...|    1392336000|Nice windscreen p...|(19899,[22,36,60,...|{\"date\": 13923360...|\n",
      "|1384719342|Musical Instruments|    5.0|    true|Rick Bennette \"Ri...|    1377648000|The primary job o...|(19899,[6,14,16,3...|{\"date\": 13776480...|\n",
      "+----------+-------------------+-------+--------+--------------------+--------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test function\n",
    "df_review = add_review_col(df_tfidf)\n",
    "\n",
    "df_review.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'{\"date\": 1392940800, \"rating\": 5.0, \"name\": \"SEAN MASLANKA\", \"text\": \"This pop filter is great. It looks and performs like a studio filter. If you\\'re recording vocals this will eliminate the pops that gets recorded when you sing.\"}'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_row = df_review.first()\n",
    "\n",
    "test_row[\"review\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group by asin, positive, then concatenate reviews, sum tfidf vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+--------+--------------------+--------------------+\n",
      "|      asin|           category|positive|             reviews|             vectors|\n",
      "+----------+-------------------+--------+--------------------+--------------------+\n",
      "|B000068NW5|Musical Instruments|   false|[{\"date\": 1394755...|[(5030,[2,3,13,23...|\n",
      "|B0002CZTIO|Musical Instruments|   false|[{\"date\": 1390608...|[(5030,[0,6,8,26,...|\n",
      "|B0002E3DGC|Musical Instruments|   false|[{\"date\": 1332374...|[(5030,[0,1,8,15,...|\n",
      "+----------+-------------------+--------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group by asin, sum tfidf_vectors\n",
    "df_grouped = df_review.groupBy(\"asin\", \"category\", \"positive\").agg(collect_list(\"review\").alias(\"reviews\"),\n",
    "                              collect_list(\"tfidf_vector\").alias(\"vectors\"))\n",
    "\n",
    "df_grouped.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get test row\n",
    "test_row = df_grouped.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_vectors = test_row[\"vectors\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(5030, {2: 1.2358, 3: 1.2506, 13: 1.6376, 23: 1.9588, 32: 2.0213, 34: 1.9, 38: 2.0213, 129: 3.1039, 157: 2.8526, 159: 2.8163, 260: 3.3759, 317: 3.4404, 398: 3.5835, 454: 3.5835, 558: 3.7506, 2109: 5.0499, 2473: 5.0499, 3809: 5.4553}),\n",
       " SparseVector(5030, {0: 1.0922, 7: 1.3202, 8: 1.43, 10: 1.5841, 34: 1.9, 59: 2.2983, 64: 2.4596, 77: 2.3873, 196: 3.0574, 220: 3.0574, 257: 3.3153, 400: 3.5835, 428: 3.5094, 542: 3.7506, 583: 3.8459, 832: 4.3567, 870: 4.3567, 1053: 4.3567, 1079: 4.539, 1124: 4.539, 3113: 5.4553}),\n",
       " SparseVector(5030, {1: 1.2212, 3: 1.2506, 6: 3.0866, 17: 1.7918, 25: 2.1595, 36: 2.1411, 41: 2.2773, 46: 7.9559, 48: 2.1411, 69: 2.3873, 93: 2.4349, 95: 2.5376, 140: 2.7812, 167: 2.9296, 171: 2.9704, 194: 2.8904, 199: 2.9704, 205: 3.1527, 206: 3.013, 219: 3.5094, 238: 3.1039, 260: 3.3759, 264: 3.2581, 284: 3.2581, 292: 3.7506, 299: 3.3759, 342: 3.3153, 359: 3.4404, 462: 3.6636, 465: 3.5835, 527: 3.7506, 579: 3.8459, 717: 4.2026, 854: 4.2026, 1057: 4.3567, 1127: 4.539, 1176: 4.539, 1641: 4.7622, 1719: 4.7622, 2172: 5.0499, 2493: 5.0499, 4491: 5.4553})]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vectors[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import SparseVector\n",
    "import numpy as np\n",
    "\n",
    "def sum_vectors(vectors):\n",
    "    \"\"\"\n",
    "    INPUT: List of SparseVectors\n",
    "    RETURN: SparseVector\n",
    "    \n",
    "    Sum list of TFIDF vectors element-wise,\n",
    "    return resulting vector\n",
    "    \n",
    "    \"\"\"\n",
    "    # check if vectors exist\n",
    "    if not vectors:\n",
    "        return None\n",
    "    \n",
    "    # iterate over vectors\n",
    "    sum_vector = vectors[0].toArray()\n",
    "    vector_size = sum_vector.shape[0]\n",
    "    \n",
    "    for i,vector in enumerate(vectors[1:]):\n",
    "        sum_vector += vector.toArray()\n",
    "        \n",
    "    # convert to sparse vector   \n",
    "    sparse_vector = SparseVector(vector_size, {i:sum_vector[i] for i in np.nonzero(sum_vector)[0]})\n",
    "    \n",
    "    return sparse_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(19899, {1: 1.2831, 2: 2.2874, 3: 2.6328, 4: 3.9307, 6: 9.2194, 8: 4.7457, 10: 16.1282, 13: 1.8166, 15: 7.399, 19: 3.979, 21: 1.9623, 24: 3.931, 26: 4.7364, 28: 4.1481, 30: 2.1634, 40: 4.6919, 41: 6.9624, 43: 5.4616, 48: 2.5104, 54: 2.4573, 57: 2.4342, 61: 2.4901, 67: 2.6843, 71: 2.7778, 73: 2.5451, 75: 2.4495, 80: 2.5988, 83: 2.7976, 84: 2.6571, 85: 2.6714, 89: 2.7291, 91: 8.1318, 94: 5.4787, 96: 2.7725, 98: 5.465, 101: 2.8673, 104: 5.5843, 107: 5.658, 108: 2.7994, 116: 2.8309, 117: 17.8971, 120: 5.8222, 125: 5.7779, 126: 28.2494, 129: 2.9317, 131: 2.9484, 142: 2.9894, 143: 2.9894, 152: 3.0072, 159: 6.9384, 161: 3.1036, 169: 3.5017, 179: 6.3935, 189: 3.2904, 190: 10.8558, 196: 3.2698, 206: 3.6959, 219: 3.4692, 231: 6.8684, 236: 3.4763, 241: 3.5469, 246: 3.4727, 248: 3.5203, 256: 3.5702, 263: 7.0481, 287: 3.5624, 306: 3.6186, 311: 3.6695, 318: 3.77, 319: 3.7139, 327: 3.7139, 364: 3.7748, 371: 3.8818, 394: 3.9895, 414: 3.9485, 418: 3.909, 463: 4.0138, 524: 8.5907, 531: 4.1377, 577: 4.24, 582: 4.2023, 606: 4.5328, 635: 4.3454, 683: 4.5024, 688: 4.389, 752: 4.5328, 758: 4.5328, 762: 4.6077, 764: 9.4762, 785: 4.5226, 790: 4.6302, 846: 4.9331, 871: 4.6416, 885: 4.9179, 911: 4.7131, 925: 4.7255, 942: 4.8309, 992: 4.8035, 1055: 4.888, 1058: 4.9967, 1149: 4.9804, 1196: 10.2004, 1199: 5.444, 1204: 41.5063, 1293: 5.1757, 1313: 5.1757, 1315: 5.1757, 1433: 5.4699, 1484: 5.3463, 1528: 5.4187, 1617: 5.524, 1797: 5.611, 1801: 5.611, 1873: 5.7402, 1920: 5.7063, 2242: 5.972, 2267: 12.126, 2275: 5.9295, 2337: 6.2172, 2676: 6.1631, 2906: 6.3349, 2975: 6.3995, 3136: 6.3995, 3279: 6.4685, 3290: 6.4685, 3425: 6.5426, 3463: 6.5426, 3569: 6.8049, 4316: 7.0281, 4413: 6.9103, 4794: 7.0281, 5834: 32.0357, 5895: 7.3158, 6541: 7.4981, 6599: 15.4425, 7355: 7.7212})"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine two vectors\n",
    "test_vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(19899, {10: 4.6081, 32: 2.221, 101: 2.8673, 174: 3.227, 275: 3.7185, 376: 4.1588, 479: 4.3368, 747: 4.5024, 1999: 5.8494, 4593: 7.0281, 5133: 7.1616, 8129: 7.7212, 9921: 8.0089})"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vectors[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(19899, {1: 2.5662, 2: 2.2874, 3: 2.6328, 4: 3.9307, 6: 16.9023, 8: 6.3276, 10: 29.9523, 11: 3.3968, 13: 5.4497, 15: 7.399, 19: 3.979, 21: 1.9623, 24: 5.8965, 26: 4.7364, 27: 4.1622, 28: 4.1481, 29: 2.1228, 30: 2.1634, 32: 2.221, 34: 2.304, 40: 4.6919, 41: 6.9624, 43: 5.4616, 45: 2.3518, 48: 2.5104, 52: 2.5536, 53: 4.9171, 54: 2.4573, 57: 2.4342, 60: 2.4955, 61: 2.4901, 66: 2.5825, 67: 5.3686, 69: 7.6736, 70: 2.5884, 71: 2.7778, 73: 2.5451, 75: 2.4495, 77: 2.5767, 79: 2.5958, 80: 2.5988, 83: 2.7976, 84: 2.6571, 85: 2.6714, 88: 5.289, 89: 2.7291, 90: 3.0987, 91: 8.1318, 93: 2.724, 94: 5.4787, 96: 2.7725, 98: 8.1975, 101: 8.6018, 104: 5.5843, 107: 8.487, 108: 2.7994, 113: 3.1414, 116: 2.8309, 117: 17.8971, 120: 5.8222, 125: 5.7779, 126: 34.5271, 129: 5.8633, 131: 2.9484, 134: 2.9633, 140: 3.0027, 142: 2.9894, 143: 2.9894, 150: 3.0768, 152: 3.0072, 159: 6.9384, 161: 6.2073, 169: 10.5052, 174: 6.454, 177: 3.1111, 179: 6.3935, 188: 3.2786, 189: 3.2904, 190: 10.8558, 196: 3.2698, 201: 3.4446, 206: 3.6959, 219: 3.4692, 220: 3.6063, 227: 3.8142, 229: 3.3872, 231: 6.8684, 234: 3.6145, 236: 3.4763, 241: 10.6406, 246: 3.4727, 248: 3.5203, 254: 3.524, 256: 3.5702, 258: 3.5054, 263: 10.5721, 265: 7.0256, 270: 3.8553, 275: 3.7185, 287: 3.5624, 292: 3.6311, 306: 3.6186, 311: 3.6695, 318: 3.77, 319: 3.7139, 327: 3.7139, 329: 3.7509, 335: 3.6914, 362: 3.7845, 364: 3.7748, 371: 3.8818, 375: 3.7604, 376: 8.3175, 383: 3.8764, 387: 4.0903, 392: 3.8193, 394: 3.9895, 406: 3.8926, 412: 4.1103, 414: 3.9485, 418: 3.909, 429: 4.0261, 444: 3.9485, 463: 4.0138, 479: 8.6737, 487: 4.1036, 524: 8.5907, 531: 4.1377, 545: 4.1447, 577: 8.48, 582: 4.2023, 606: 4.5328, 635: 4.3454, 683: 4.5024, 688: 4.389, 729: 9.451, 747: 4.5024, 752: 4.5328, 758: 4.5328, 762: 4.6077, 764: 9.4762, 785: 4.5226, 790: 4.6302, 823: 4.6767, 845: 4.6532, 846: 4.9331, 871: 4.6416, 885: 4.9179, 888: 4.9486, 903: 4.7131, 911: 4.7131, 925: 4.7255, 942: 4.8309, 953: 4.79, 984: 4.8734, 992: 4.8035, 1055: 4.888, 1058: 4.9967, 1104: 5.0471, 1149: 4.9804, 1196: 10.2004, 1199: 5.444, 1204: 41.5063, 1248: 5.1955, 1293: 5.1757, 1313: 5.1757, 1315: 5.1757, 1407: 5.2789, 1433: 5.4699, 1437: 5.3699, 1484: 5.3463, 1492: 5.3463, 1528: 5.4187, 1536: 5.3699, 1588: 5.524, 1616: 5.611, 1617: 5.524, 1639: 5.524, 1792: 5.5812, 1797: 5.611, 1801: 5.611, 1851: 5.611, 1873: 5.7402, 1920: 5.7063, 1999: 5.8494, 2242: 5.972, 2267: 12.126, 2275: 5.9295, 2337: 6.2172, 2401: 5.972, 2466: 6.063, 2658: 6.2172, 2676: 6.1631, 2906: 6.3349, 2975: 6.3995, 3136: 6.3995, 3279: 6.4685, 3290: 6.4685, 3425: 6.5426, 3463: 6.5426, 3569: 6.8049, 3573: 6.8049, 3816: 6.7096, 3831: 6.7096, 4316: 7.0281, 4413: 6.9103, 4593: 7.0281, 4753: 7.0281, 4794: 7.0281, 5133: 7.1616, 5834: 32.0357, 5895: 7.3158, 6083: 7.4981, 6541: 7.4981, 6599: 15.4425, 7355: 7.7212, 7865: 7.7212, 8129: 7.7212, 9921: 8.0089, 12932: 8.4144, 16735: 8.4144, 17767: 8.4144})"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test sum of first two vectors\n",
    "sum_vectors(test_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to sum vectors over DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import VectorUDT\n",
    "\n",
    "# sum vectors\n",
    "def add_vectors_sum(df):\n",
    "    # create udf\n",
    "    sum_vector_udf = udf(lambda vectors: sum_vectors(vectors), VectorUDT())\n",
    "\n",
    "    # create new column with review\n",
    "    df_vectors_summed = df.withColumn(\"tfidf_vectors_sum\", sum_vector_udf(\"vectors\")).drop(\"vectors\")\n",
    "    \n",
    "    return df_vectors_summed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+--------+--------------------+--------------------+\n",
      "|      asin|           category|positive|             reviews|   tfidf_vectors_sum|\n",
      "+----------+-------------------+--------+--------------------+--------------------+\n",
      "|B000068NW5|Musical Instruments|   false|[{\"date\": 1394755...|(5030,[0,1,2,3,6,...|\n",
      "|B0002CZTIO|Musical Instruments|   false|[{\"date\": 1390608...|(5030,[0,6,8,26,4...|\n",
      "|B0002E3DGC|Musical Instruments|   false|[{\"date\": 1332374...|(5030,[0,1,8,15,1...|\n",
      "+----------+-------------------+--------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "df_vectors_summed = add_vectors_sum(df_grouped)\n",
    "\n",
    "df_vectors_summed.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add top terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sentimentAnalysis.dataProcessing import add_pos_neg_features\\ndf_terms = add_pos_neg_features(df_vectors_summed, vocab_pos, vocab_neg, n=15).drop(\"tfidf_vectors_sum\")\\ndf_terms.show(3)\\n'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get top terms\n",
    "\"\"\"\n",
    "from sentimentAnalysis.dataProcessing import add_pos_neg_features\n",
    "df_terms = add_pos_neg_features(df_vectors_summed, vocab_pos, vocab_neg, n=15).drop(\"tfidf_vectors_sum\")\n",
    "df_terms.show(3)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_extract_top_features(tfidf_vector, vocab, n):\n",
    "    \"\"\"\n",
    "    INPUT: SparseVector, List, Int\n",
    "    RETURN: List\n",
    "\n",
    "    Take in TFIDF vector, vocabulary for vector,\n",
    "    and number of terms. Return top n terms\n",
    "\n",
    "    \"\"\"\n",
    "    # note - tfidf elements are pre-sorted by importance\n",
    "    term_indices = tfidf_vector.indices[-n:]\n",
    "\n",
    "    # Map features to terms\n",
    "    features = [vocab[i] for i in term_indices]\n",
    "\n",
    "    return features\n",
    "\n",
    "def test_add_top_features(df, vocab, n=10):\n",
    "    \"\"\"\n",
    "    INPUT: PySpark DataFrame, List, Int\n",
    "    RETURN: PySpark DataFrame\n",
    "\n",
    "    Take in DataFrame with TFIDF vectors, list of vocabulary words,\n",
    "    and number of features to extract. Map top features from TFIDF\n",
    "    vectors to vocabulary terms. Return new DataFrame with terms\n",
    "\n",
    "    \"\"\"\n",
    "    # Create udf function to extract top n features\n",
    "    extract_features_udf = udf(lambda x: test_extract_top_features(x, vocab, n))\n",
    "\n",
    "    # Apply udf, create new df with features column\n",
    "    df_features = df.withColumn(\"topFeatures\",\n",
    "                                    extract_features_udf(df[\"tfidf_vectors_sum\"]))\n",
    "\n",
    "\n",
    "    return df_features\n",
    "\n",
    "def test_add_pos_neg_features(df, vocab_pos, vocab_neg, n=10):\n",
    "    \"\"\"\n",
    "    INPUT: Spark DataFrame, List, List, Int\n",
    "    RETURN: Spark DataFrame\n",
    "\n",
    "    Take in DataFrame grouped by asin, positive with tfidf vectors summed.\n",
    "    Extract top positive and negative terms from each group, add features column\n",
    "\n",
    "    \"\"\"\n",
    "    # split dataframe on postitive\n",
    "    df_pos = df.where(df.positive==True)\n",
    "    df_neg = df.where(df.positive==False)\n",
    "\n",
    "    # add features\n",
    "    df_pos_terms = test_add_top_features(df_pos, vocab_pos, n)\n",
    "    df_neg_terms = test_add_top_features(df_neg, vocab_neg, n)\n",
    "\n",
    "    return df_pos_terms.unionAll(df_neg_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add terms\n",
    "# note - udf function that relies on local module functions does not work b/c module does not exist on workers\n",
    "\n",
    "df_terms = test_add_pos_neg_features(df_vectors_summed, vocab_pos, vocab_neg, n=15).drop(\"tfidf_vectors_sum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce, add posFeatures and negFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+--------+--------------------+--------------------+\n",
      "|      asin|           category|positive|             reviews|         topFeatures|\n",
      "+----------+-------------------+--------+--------------------+--------------------+\n",
      "|B0014IEBM0|Musical Instruments|    true|[{\"date\": 1251072...|[portables, subtl...|\n",
      "|B001EC5ECW|Musical Instruments|    true|[{\"date\": 1385337...|[recommendation, ...|\n",
      "|B001L8IJ0I|Musical Instruments|    true|[{\"date\": 1362355...|[mg, dsl, glory, ...|\n",
      "+----------+-------------------+--------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_terms.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+--------------------+--------------------+\n",
      "|      asin|           category|             reviews|         topFeatures|\n",
      "+----------+-------------------+--------------------+--------------------+\n",
      "|B0002CZSJY|Musical Instruments|[WrappedArray({\"d...|[[usese, bubblych...|\n",
      "+----------+-------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group by asin\n",
    "df_terms_grouped = df_terms.groupBy(\"asin\", \"category\").agg(collect_list(\"reviews\").alias(\"reviews\"),\n",
    "                              collect_list(\"topFeatures\").alias(\"topFeatures\"))\n",
    "\n",
    "df_terms_grouped.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+\n",
      "|      asin|          negReviews|         negFeatures|\n",
      "+----------+--------------------+--------------------+\n",
      "|B0014IEBM0|[{\"date\": 1251072...|[portables, subtl...|\n",
      "|B001EC5ECW|[{\"date\": 1385337...|[recommendation, ...|\n",
      "|B001L8IJ0I|[{\"date\": 1362355...|[mg, dsl, glory, ...|\n",
      "+----------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_terms.select(\"asin\",\n",
    "                col(\"reviews\").alias(\"negReviews\"), \n",
    "                col(\"topFeatures\").alias(\"negFeatures\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create review column\n",
    "def collapse_reviews_terms(df):\n",
    "    # split dataframe\n",
    "    df_pos = df.where(df.positive==True)\n",
    "    df_pos = df_pos.select(col(\"asin\"),\n",
    "                           col(\"category\"),\n",
    "                           col(\"reviews\").alias(\"posReviews\"), \n",
    "                           col(\"topFeatures\").alias(\"posFeatures\"))\n",
    "    \n",
    "    df_neg = df.where(df.positive==False)\n",
    "    df_neg = df_neg.select(col(\"asin\"),\n",
    "                           col(\"reviews\").alias(\"negReviews\"), \n",
    "                           col(\"topFeatures\").alias(\"negFeatures\"))\n",
    "    \n",
    "    \n",
    "    # get asin \n",
    "    df_asin = df.select(\"asin\").distinct()\n",
    "    \n",
    "    \n",
    "    # join dataframes\n",
    "    df_final = df_asin.join(df_pos, df_asin.asin==df_neg.asin, 'outer').drop(df_pos.asin)\n",
    "    df_final = df_final.join(df_neg, df_final.asin==df_neg.asin, 'outer').drop(df_neg.asin)\n",
    "    \n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_final = collapse_reviews_terms(df_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|      asin|           category|          posReviews|         posFeatures|          negReviews|         negFeatures|\n",
      "+----------+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|B000MWWT6E|Musical Instruments|[{\"date\": 1275696...|[dang, pony, tick...|                null|                null|\n",
      "|B000P5NXWM|Musical Instruments|[{\"date\": 1301702...|[replacements, sa...|[{\"date\": 1374969...|[tone, purchased,...|\n",
      "|B000RNB720|Musical Instruments|[{\"date\": 1383264...|[investigated, 7a...|[{\"date\": 1376006...|[can't, big, usin...|\n",
      "+----------+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modules needed\n",
    "#from pyspark.sql.functions import udf\n",
    "#from pyspark.sql.types import ArrayType, StringType\n",
    "#from sentimentAnalysis import dataProcessing as dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## Clear Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[asin: string, overall: double, reviewerName: string, unixReviewTime: bigint, reviewText: string, category: string]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unpersist old dataframes\n",
    "df_cat.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# end session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
